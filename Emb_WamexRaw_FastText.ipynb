{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WAMEX reports: pre-process data and train the embedding with FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-09 13:17:16,378 : INFO : START - FastText Embeddings for all wamex reports\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/majiga/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/majiga/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/majiga/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = list(get_stop_words('en'))         # 174 stopwords\n",
    "nltk_words = list(stopwords.words('english'))   # 153 stopwords\n",
    "stop_words.extend(nltk_words)                   # 353 in total\n",
    "\n",
    "import re\n",
    "import os, glob, codecs\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.info('START - FastText Embeddings for all wamex reports')\n",
    "\n",
    "WAMEX_DATA_FOLDER = r\"/Users/majiga/Documents/wamex/data/wamex_xml\"\n",
    "\n",
    "MODEL_FILE = r\"Vectors/fastText_wamex_raw.model\" # bin file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-09 13:23:13,426 : INFO : loading FastText object from Vectors/fastText_wamex_raw.model\n",
      "2018-11-09 13:23:13,755 : INFO : loading wv recursively from Vectors/fastText_wamex_raw.model.wv.* with mmap=None\n",
      "2018-11-09 13:23:13,756 : INFO : setting ignored attribute vectors_norm to None\n",
      "2018-11-09 13:23:13,757 : INFO : setting ignored attribute vectors_vocab_norm to None\n",
      "2018-11-09 13:23:13,757 : INFO : setting ignored attribute vectors_ngrams_norm to None\n",
      "2018-11-09 13:23:13,757 : INFO : setting ignored attribute buckets_word to None\n",
      "2018-11-09 13:23:13,758 : INFO : loading vocabulary recursively from Vectors/fastText_wamex_raw.model.vocabulary.* with mmap=None\n",
      "2018-11-09 13:23:13,758 : INFO : loading trainables recursively from Vectors/fastText_wamex_raw.model.trainables.* with mmap=None\n",
      "2018-11-09 13:23:13,758 : INFO : loaded Vectors/fastText_wamex_raw.model\n",
      "2018-11-09 13:23:13,774 : INFO : precomputing L2-norms of word weight vectors\n",
      "2018-11-09 13:23:13,778 : INFO : precomputing L2-norms of ngram weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=8562, size=100, alpha=0.025)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('au', 0.7181254625320435),\n",
       " ('copper', 0.6624714732170105),\n",
       " ('nickel', 0.6557887196540833),\n",
       " ('metal', 0.6335586905479431),\n",
       " ('precious', 0.6275296807289124),\n",
       " ('arsenic', 0.6186712384223938),\n",
       " ('antimony', 0.6167358756065369),\n",
       " ('tungsten', 0.6000398993492126),\n",
       " ('historically', 0.5887951850891113),\n",
       " ('resolute', 0.5784502029418945)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "model_fasttext = FastText.load(MODEL_FILE)\n",
    "print(new_model)\n",
    "\n",
    "model_fasttext.wv.most_similar('gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('iron', 0.7587395310401917), ('ironcap', 0.6327366828918457), ('marra', 0.5913161039352417), ('brockman', 0.5766406655311584), ('bif', 0.5754503607749939), ('sinosteel', 0.5720453262329102), ('nammuldi', 0.5704472661018372), ('manganese', 0.5699800252914429), ('banded', 0.5622855424880981), ('finnerty', 0.5618066787719727)]\n",
      "[('marra', 0.6866266131401062), ('brockman', 0.6819794774055481), ('hematite', 0.6760455369949341), ('banded', 0.6720453500747681), ('detritals', 0.6715987920761108), ('bid', 0.6652620434761047), ('bif', 0.6647725701332092), ('ore', 0.6554528474807739), ('manganese', 0.6512892842292786), ('dso', 0.6481667160987854)]\n",
      "[('esperance', 0.47153985500335693), ('bunbury', 0.45931476354599), ('pannawonica', 0.45913761854171753), ('karratha', 0.44624924659729004), ('geraldton', 0.4430530369281769), ('coolgardie', 0.42476966977119446), ('balladonia', 0.42342591285705566), ('vetters', 0.41675683856010437), ('hyden', 0.4077901244163513), ('bulong', 0.404255747795105)]\n",
      "[ 0.05621057  0.06094638  0.02233304 -0.01322563  0.28116795 -0.4403501\n",
      "  0.16161472  0.30070785  0.16008513 -0.2022848  -0.25012115 -0.05608174\n",
      " -0.3637988  -0.1513833  -0.15876748 -0.6227062   0.08043403 -0.28495207\n",
      " -0.2422285  -0.07330472  0.30681527 -0.5922584   0.3766731  -0.5793361\n",
      "  0.25363004 -0.09410779 -0.15475652 -0.49917784 -0.22074729 -0.37053573\n",
      " -0.2789399   0.11776371  0.03855462  0.11382677  0.5629924   0.42633075\n",
      "  0.22135681 -0.25894874 -0.29392913  0.07488628 -0.07928842  0.4643137\n",
      " -0.07193653  0.31285957 -0.20839134  0.04095297 -0.5154743  -0.12767124\n",
      "  0.20314535 -0.21444434 -0.14694737  0.00273706 -0.09381776 -0.1465394\n",
      "  0.49311695 -0.24150413  0.19585465  0.08099093 -0.36161432 -0.2565246\n",
      "  0.07621556  0.21230143  0.39075595  0.00083028  0.14124975  0.29770163\n",
      "  0.23033653  0.41169688 -0.18276638 -0.35430992 -0.07166957  0.37378874\n",
      " -0.33961865  0.42070457  0.2641345  -0.18017167 -0.40076238  0.2020824\n",
      "  0.02719929  0.28642207 -0.18780518 -0.54637575 -0.24583445 -0.05076355\n",
      "  0.10112357  0.15961169  0.09025458  0.03068086 -0.12750274  0.15714966\n",
      " -0.24663375  0.04165075 -0.11396042  0.01046028 -0.06396001  0.0578107\n",
      " -0.3253031  -0.19045162 -0.14661692 -0.3476625 ]\n"
     ]
    }
   ],
   "source": [
    "# model_fasttext.wv.most_similar('kalgoorlie')\n",
    "\n",
    "print(model_fasttext.wv.most_similar('iron-ore'))\n",
    "\n",
    "print(model_fasttext.wv.most_similar('iron', topn=10))\n",
    "\n",
    "\n",
    "print(model_fasttext.wv.most_similar(positive=['kalgoorlie','iron-ore'],negative=['gold']))\n",
    "\n",
    "print(new_model.wv['gold'])  # numpy vector of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8562\n"
     ]
    }
   ],
   "source": [
    "words = list(model_fasttext.wv.vocab)\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embeddings\n",
    "Note: Or load already trained model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def tokenize_and_lemmatize(input_text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token    \n",
    "    tokens = [word for sent in nltk.sent_tokenize(input_text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z-]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    # Lemma\n",
    "    lemmas = [lemmatizer.lemmatize(t) for t in filtered_tokens]\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Read a txt file and return sentences\n",
    "\"\"\"\n",
    "def read_clean_file(filename):\n",
    "       \n",
    "    with codecs.open(filename, \"r\", encoding='utf-8', errors='ignore') as f:\n",
    "        data = f.read()\n",
    "    if (len(data) < 10):\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    data_cleaned = [] \n",
    "    # remove stop words\n",
    "    data = data.lower()\n",
    "    data = data.replace('-', ' ')\n",
    "    data = data.replace(',', ' ')\n",
    "    data = data.replace('\\\\', ' ')\n",
    "    data = data.replace('/', ' ')\n",
    "    \n",
    "    for w in data.split():\n",
    "        if (w not in stop_words):\n",
    "            data_cleaned.append(w)\n",
    "    \n",
    "    #print('CLEAN DATA')\n",
    "    #print(' '.join(data_cleaned))\n",
    "    \n",
    "    # lemmatize words in each sentences\n",
    "    data_lemmatized = tokenize_and_lemmatize(' '.join(data_cleaned))\n",
    "    \n",
    "    return data_lemmatized\n",
    "\n",
    "# READ WAMEX REPORTS FROM THE DATA FOLDER\n",
    "logging.info('START - Read cleaned wamex reports in ' + WAMEX_DATA_FOLDER)\n",
    "reports_data = []\n",
    "for filename in glob.glob(os.path.join(WAMEX_DATA_FOLDER, '*.json')):\n",
    "    # do your stuff\n",
    "    if (read_clean_file(filename) is not None):\n",
    "        reports_data.append(read_clean_file(filename).split())\n",
    "logging.info('END - Read wamex reports')\n",
    "\n",
    "# Counting tokens\n",
    "count_tokens = 0\n",
    "for data in reports_data:\n",
    "    count_tokens += len(data)\n",
    "print(count_tokens)\n",
    "# 42650553 tokens\n",
    "\n",
    "# Train the model\n",
    "logging.info(\"START - Train the fastText model\")\n",
    "model_fasttext = FastText(reports_data, size=100, window=5, min_count=300, workers=4, sg=1)\n",
    "\n",
    "# min_count=100 => training on a 246019540 raw words (182291433 effective words) took 817.9s, 222873 effective words/s if 100+ frequency\n",
    "\n",
    "logging.info('END - FastText Embeddings for all wamex reports')\n",
    "\n",
    "# Count the words in the vocabulary\n",
    "print(len(model_fasttext.wv.vocab)) # 8562\n",
    "\n",
    "# save model\n",
    "model_fasttext.save(MODEL_FILE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
