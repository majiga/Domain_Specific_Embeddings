{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WAMEX entities-only embedding with FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-08 10:21:58,889 : INFO : START - FastText Embeddings for entities only file\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "MODEL_FILE_NAME = r\"Vectors/FastText_wamex_terms.model\"\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.info('START - FastText Embeddings for entities only file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the entities only file and Train the FastText model\n",
    "Note: Or load the trained model in the next line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-08 10:22:01,681 : INFO : START - Train the fastText model\n",
      "2018-11-08 10:22:01,682 : INFO : collecting all words and their counts\n",
      "2018-11-08 10:22:01,683 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-11-08 10:22:01,817 : INFO : PROGRESS: at sentence #10000, processed 1009032 words, keeping 1517 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2772122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-08 10:22:01,936 : INFO : PROGRESS: at sentence #20000, processed 1896449 words, keeping 1723 word types\n",
      "2018-11-08 10:22:02,050 : INFO : PROGRESS: at sentence #30000, processed 2689435 words, keeping 1845 word types\n",
      "2018-11-08 10:22:02,063 : INFO : collected 1853 word types from a corpus of 2772122 raw words and 31328 sentences\n",
      "2018-11-08 10:22:02,063 : INFO : Loading a fresh vocabulary\n",
      "2018-11-08 10:22:02,066 : INFO : min_count=100 retains 838 unique words (45% of original 1853, drops 1015)\n",
      "2018-11-08 10:22:02,066 : INFO : min_count=100 leaves 2747343 word corpus (99% of original 2772122, drops 24779)\n",
      "2018-11-08 10:22:02,069 : INFO : deleting the raw counts dictionary of 1853 items\n",
      "2018-11-08 10:22:02,069 : INFO : sample=0.001 downsamples 74 most-common words\n",
      "2018-11-08 10:22:02,070 : INFO : downsampling leaves estimated 1532704 word corpus (55.8% of prior 2747343)\n",
      "2018-11-08 10:22:02,163 : INFO : estimated required memory for 838 words, 16345 buckets and 100 dimensions: 7920824 bytes\n",
      "2018-11-08 10:22:02,163 : INFO : resetting layer weights\n",
      "2018-11-08 10:22:02,549 : INFO : Total number of ngrams is 16345\n",
      "2018-11-08 10:22:02,685 : INFO : training model with 4 workers on 838 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-11-08 10:22:03,702 : INFO : EPOCH 1 - PROGRESS: at 12.04% examples, 220961 words/s, in_qsize 6, out_qsize 1\n",
      "2018-11-08 10:22:04,719 : INFO : EPOCH 1 - PROGRESS: at 27.47% examples, 232760 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 10:22:05,734 : INFO : EPOCH 1 - PROGRESS: at 42.70% examples, 236031 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 10:22:06,741 : INFO : EPOCH 1 - PROGRESS: at 58.69% examples, 238194 words/s, in_qsize 6, out_qsize 1\n",
      "2018-11-08 10:22:07,749 : INFO : EPOCH 1 - PROGRESS: at 75.78% examples, 238395 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 10:22:08,758 : INFO : EPOCH 1 - PROGRESS: at 92.19% examples, 236307 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 10:22:09,098 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-08 10:22:09,123 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-08 10:22:09,146 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-08 10:22:09,167 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-08 10:22:09,167 : INFO : EPOCH - 1 : training on 2772122 raw words (1532596 effective words) took 6.5s, 236533 effective words/s\n",
      "2018-11-08 10:22:10,180 : INFO : EPOCH 2 - PROGRESS: at 11.63% examples, 216397 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 10:22:11,183 : INFO : EPOCH 2 - PROGRESS: at 26.44% examples, 226388 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 10:22:12,184 : INFO : EPOCH 2 - PROGRESS: at 40.80% examples, 227827 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 10:22:13,213 : INFO : EPOCH 2 - PROGRESS: at 57.04% examples, 233133 words/s, in_qsize 8, out_qsize 1\n",
      "2018-11-08 10:22:14,242 : INFO : EPOCH 2 - PROGRESS: at 74.95% examples, 235726 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 10:22:15,262 : INFO : EPOCH 2 - PROGRESS: at 93.30% examples, 238218 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 10:22:15,530 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-08 10:22:15,549 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-08 10:22:15,571 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-08 10:22:15,591 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-08 10:22:15,591 : INFO : EPOCH - 2 : training on 2772122 raw words (1532917 effective words) took 6.4s, 238699 effective words/s\n",
      "2018-11-08 10:22:16,614 : INFO : EPOCH 3 - PROGRESS: at 13.09% examples, 235676 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 10:22:17,622 : INFO : EPOCH 3 - PROGRESS: at 27.97% examples, 241197 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 10:22:18,637 : INFO : EPOCH 3 - PROGRESS: at 43.80% examples, 243148 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 10:22:19,660 : INFO : EPOCH 3 - PROGRESS: at 60.28% examples, 242657 words/s, in_qsize 6, out_qsize 1\n",
      "2018-11-08 10:22:20,673 : INFO : EPOCH 3 - PROGRESS: at 76.84% examples, 240879 words/s, in_qsize 8, out_qsize 0\n",
      "2018-11-08 10:22:21,702 : INFO : EPOCH 3 - PROGRESS: at 93.84% examples, 238605 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 10:22:21,938 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-08 10:22:21,958 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-08 10:22:21,981 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-08 10:22:22,001 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-08 10:22:22,001 : INFO : EPOCH - 3 : training on 2772122 raw words (1533103 effective words) took 6.4s, 239287 effective words/s\n",
      "2018-11-08 10:22:23,006 : INFO : EPOCH 4 - PROGRESS: at 11.63% examples, 218033 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 10:22:24,026 : INFO : EPOCH 4 - PROGRESS: at 26.44% examples, 225475 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 10:22:25,059 : INFO : EPOCH 4 - PROGRESS: at 40.80% examples, 224967 words/s, in_qsize 8, out_qsize 0\n",
      "2018-11-08 10:22:26,074 : INFO : EPOCH 4 - PROGRESS: at 55.43% examples, 226426 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 10:22:27,120 : INFO : EPOCH 4 - PROGRESS: at 71.79% examples, 224013 words/s, in_qsize 8, out_qsize 0\n",
      "2018-11-08 10:22:28,129 : INFO : EPOCH 4 - PROGRESS: at 87.33% examples, 223199 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 10:22:28,771 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-08 10:22:28,790 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-08 10:22:28,813 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-08 10:22:28,832 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-08 10:22:28,833 : INFO : EPOCH - 4 : training on 2772122 raw words (1533096 effective words) took 6.8s, 224485 effective words/s\n",
      "2018-11-08 10:22:29,873 : INFO : EPOCH 5 - PROGRESS: at 11.63% examples, 210499 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 10:22:30,883 : INFO : EPOCH 5 - PROGRESS: at 26.44% examples, 222457 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 10:22:31,905 : INFO : EPOCH 5 - PROGRESS: at 40.80% examples, 223707 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 10:22:32,914 : INFO : EPOCH 5 - PROGRESS: at 55.43% examples, 225528 words/s, in_qsize 6, out_qsize 1\n",
      "2018-11-08 10:22:33,936 : INFO : EPOCH 5 - PROGRESS: at 72.11% examples, 225432 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 10:22:34,977 : INFO : EPOCH 5 - PROGRESS: at 88.21% examples, 224171 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 10:22:35,570 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-08 10:22:35,594 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-08 10:22:35,615 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-08 10:22:35,632 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-08 10:22:35,633 : INFO : EPOCH - 5 : training on 2772122 raw words (1532145 effective words) took 6.8s, 225392 effective words/s\n",
      "2018-11-08 10:22:35,633 : INFO : training on a 13860610 raw words (7663857 effective words) took 32.9s, 232606 effective words/s\n",
      "2018-11-08 10:22:35,690 : INFO : END - FastText Embeddings for entities\n",
      "2018-11-08 10:22:35,691 : INFO : saving FastText object under Vectors/FastText_wamex_terms.model, separately None\n",
      "2018-11-08 10:22:35,691 : INFO : not storing attribute vectors_norm\n",
      "2018-11-08 10:22:35,692 : INFO : not storing attribute vectors_vocab_norm\n",
      "2018-11-08 10:22:35,692 : INFO : not storing attribute vectors_ngrams_norm\n",
      "2018-11-08 10:22:35,693 : INFO : not storing attribute buckets_word\n",
      "2018-11-08 10:22:35,784 : INFO : saved Vectors/FastText_wamex_terms.model\n"
     ]
    }
   ],
   "source": [
    "FILE_TO_READ = r\"/Users/majiga/Documents/wamex/WAMEX_geological_entities_allfiles.txt\"\n",
    "\n",
    "sentences = \"\"\n",
    "with open(FILE_TO_READ, 'r') as f:\n",
    "    sentences = f.readlines()\n",
    "print(len(sentences))   # 31328 files=lines of 28910989 words=tokens\n",
    "\n",
    "data = []\n",
    "count_terms = 0\n",
    "for s in sentences:\n",
    "    arr = s.split(', ')\n",
    "    words = []\n",
    "    for w in arr:\n",
    "        a = w.strip().replace(' ', '-')\n",
    "        words.append(a)\n",
    "    data.append(words)\n",
    "    count_terms += len(words) # 2,772,122\n",
    "\n",
    "print(count_terms)\n",
    "\n",
    "logging.info(\"START - Train the fastText model\")\n",
    "# Skip gram model FastText model \n",
    "# min_count of 100 --- min number of word occurrence\n",
    "# number of negatives sampled [5]\n",
    "model_fasttext = FastText(data, size=100, window=5, min_count=100, workers=4, sg=1)\n",
    "logging.info('END - FastText Embeddings for entities')\n",
    "\n",
    "# save model\n",
    "model_fasttext.save(MODEL_FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the existing model without training again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-08 10:22:48,986 : INFO : loading FastText object from Vectors/FastText_wamex_terms.model\n",
      "2018-11-08 10:22:49,063 : INFO : loading wv recursively from Vectors/FastText_wamex_terms.model.wv.* with mmap=None\n",
      "2018-11-08 10:22:49,064 : INFO : setting ignored attribute vectors_norm to None\n",
      "2018-11-08 10:22:49,064 : INFO : setting ignored attribute vectors_vocab_norm to None\n",
      "2018-11-08 10:22:49,065 : INFO : setting ignored attribute vectors_ngrams_norm to None\n",
      "2018-11-08 10:22:49,065 : INFO : setting ignored attribute buckets_word to None\n",
      "2018-11-08 10:22:49,066 : INFO : loading vocabulary recursively from Vectors/FastText_wamex_terms.model.vocabulary.* with mmap=None\n",
      "2018-11-08 10:22:49,066 : INFO : loading trainables recursively from Vectors/FastText_wamex_terms.model.trainables.* with mmap=None\n",
      "2018-11-08 10:22:49,066 : INFO : loaded Vectors/FastText_wamex_terms.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.11394285 -0.2954462  -0.00161281 -0.13292015  0.16249023 -0.29896724\n",
      " -0.11928976 -0.09642559  0.03694298 -0.01508666  0.3389508   0.16575722\n",
      "  0.35612342  0.3288069   0.3662248   0.1309997  -0.08535857  0.05184054\n",
      "  0.08872069  0.04750683  0.10573867 -0.19169861  0.14181891 -0.02243489\n",
      " -0.10144629 -0.35596833  0.05233613  0.01030986  0.02784077  0.05750883\n",
      " -0.23626123 -0.14224444 -0.01390793  0.17340924 -0.03772602  0.3135555\n",
      "  0.03876122  0.06076474  0.08404154 -0.12702513  0.01460998  0.06024339\n",
      " -0.30078572  0.11498134  0.15024103  0.35172573  0.06366682  0.16782545\n",
      " -0.16606937  0.37653887 -0.13072525  0.23260714 -0.05100095  0.17505772\n",
      "  0.20597345 -0.09268837 -0.28350693 -0.39543292  0.38256982 -0.03765872\n",
      "  0.37133476 -0.13598533  0.49424398 -0.11985529  0.12764889 -0.061955\n",
      "  0.12554361 -0.04529578 -0.03425958  0.05043591  0.39497897 -0.19723797\n",
      "  0.16822003  0.22117823 -0.05482369  0.17574376 -0.563747   -0.01264164\n",
      "  0.4310346   0.11678924 -0.0020106   0.10945526 -0.13770595 -0.10630382\n",
      " -0.13740386 -0.18754508  0.08341665 -0.10994918  0.2353396   0.02561591\n",
      "  0.14844412 -0.17687163  0.11428332 -0.04769708 -0.00588365  0.08911285\n",
      "  0.06814522 -0.02853242  0.09323785  0.11617226]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/majiga/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "model = FastText.load(MODEL_FILE_NAME)\n",
    "print(model['gold'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-08 10:19:54,085 : INFO : precomputing L2-norms of word weight vectors\n",
      "2018-11-08 10:19:54,087 : INFO : precomputing L2-norms of ngram weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "838\n",
      "[('surface-gold', 0.6701695919036865), ('mineralisation', 0.6299868822097778), ('gold-mineral', 0.6059813499450684), ('mineralization', 0.585066020488739), ('kalgoorlie', 0.5806409120559692), ('metal', 0.5685122609138489), ('western-australia', 0.5583513379096985), ('nickel', 0.5336951017379761), ('greenstone-belt', 0.5104318857192993), ('archaean', 0.5011576414108276)]\n",
      "[('iron', 0.7401604056358337), ('hematite', 0.6724737286567688), ('west-angelas', 0.6016417145729065), ('marandoo', 0.5700218081474304), ('mount-jackson', 0.5502362847328186), ('windarling', 0.5446536540985107), ('hamersley', 0.5273263454437256), ('martite', 0.5261359810829163), ('banded-iron-formation', 0.5159679055213928), ('tallering-peak', 0.5107094645500183)]\n",
      "[('iron-ore', 0.7401604056358337), ('hematite', 0.6376545429229736), ('west-angelas', 0.6171693205833435), ('hamersley', 0.5958882570266724), ('marandoo', 0.5773637294769287), ('mount-sylvia-formation', 0.5698187351226807), ('wittenoom-formation', 0.5667811036109924), ('banded-iron-formation', 0.5623644590377808), ('macleod-member', 0.5620712041854858), ('mount-newman-member', 0.5543608069419861)]\n",
      "[('hematite', 0.5169974565505981), ('west-angelas', 0.500630259513855), ('marandoo', 0.4826710820198059), ('martite', 0.48114416003227234), ('windarling', 0.47811490297317505), ('yerecoin', 0.46739333868026733), ('iron', 0.4541175961494446), ('bullfinch', 0.429188996553421), ('mount-jackson', 0.42757534980773926), ('sodium-nitrate', 0.415027916431427)]\n",
      "[('coolgardie', 0.6710657477378845), ('kalgoorlie-terrane', 0.6467024087905884), ('norseman', 0.6336493492126465), ('kalgoorlie-group', 0.6046400666236877), ('broad-arrow', 0.5884636044502258), ('gold', 0.5806409120559692), ('kurnalpi', 0.5772097706794739), ('kanowna', 0.5704938173294067), ('kambalda', 0.5703580379486084), ('widgiemooltha', 0.5480405688285828)]\n",
      "[ 0.03716844 -0.11428003  0.03078874  0.01185397  0.03143578 -0.0351928\n",
      "  0.043697    0.03993947  0.11504221  0.12388728  0.6519895  -0.12335224\n",
      "  0.22420047  0.09388349  0.09051771  0.00448232 -0.02004414  0.065958\n",
      "  0.25936174 -0.09149348 -0.01659291 -0.12134574  0.0390336   0.08410304\n",
      " -0.1402368   0.14696035  0.14084896 -0.15676118 -0.00611983 -0.15680026\n",
      " -0.02811962 -0.42666024 -0.2697551   0.30475542  0.1990458   0.47340077\n",
      " -0.15494959 -0.16148943 -0.0644084  -0.00743076 -0.01772034 -0.03692434\n",
      " -0.20180993  0.11331889 -0.04300809  0.25911403  0.13423441  0.32109508\n",
      " -0.06834163 -0.44577086  0.2623982   0.24080504 -0.18244253  0.32080194\n",
      "  0.16327646 -0.09035167 -0.1096476  -0.34710395  0.21516514  0.1302835\n",
      "  0.08447825 -0.32557282  0.2820716   0.16508904  0.15872724  0.24544674\n",
      " -0.00270109 -0.08544402  0.0321154   0.03014699 -0.09593458 -0.13547398\n",
      " -0.12138215 -0.04683514 -0.23474985  0.3329139  -0.09486051  0.2273387\n",
      " -0.1711737  -0.02299727  0.06335721  0.30392593 -0.3183864   0.23847023\n",
      "  0.02398486  0.08230156 -0.12763464 -0.17779677 -0.06446834 -0.25509784\n",
      "  0.18815662  0.09929607  0.21412505 -0.0652205  -0.27413738  0.17282443\n",
      "  0.43374994  0.21626176 -0.0199796   0.00818129]\n"
     ]
    }
   ],
   "source": [
    "#print(model_fasttext.wv.vocab)\n",
    "print(len(model.wv.vocab))\n",
    "# 838 words 100+ freq\n",
    "\n",
    "print(model.wv.most_similar('gold'))\n",
    "print(model.wv.most_similar('iron-ore'))\n",
    "print(model.wv.most_similar('iron', topn=10))\n",
    "print(model.wv.most_similar(positive=['kalgoorlie','iron-ore'],negative=['gold']))\n",
    "\n",
    "print(model.wv.most_similar('kalgoorlie'))\n",
    "print(model.wv['gold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
