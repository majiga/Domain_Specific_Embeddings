{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WAMEX reports: pre-process data and train the embedding with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-08 09:37:14,706 : INFO : START - FastText Embeddings for all wamex reports\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/majiga/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/majiga/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/majiga/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import codecs\n",
    "import glob, os\n",
    "import gensim\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.info('START - FastText Embeddings for all wamex reports')\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = list(get_stop_words('en'))         # 174 stopwords\n",
    "nltk_words = list(stopwords.words('english'))   # 153 stopwords\n",
    "stop_words.extend(nltk_words)                   # 353 in total\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "WAMEX_DATA_FOLDER = r\"/Users/majiga/Documents/wamex/data/wamex_xml/\"\n",
    "\n",
    "MODEL_FILE = r\"Vectors/word2vec_wamex_all_raw.model\" # bin file\n",
    "\n",
    "def tokenize_and_lemmatize(input_text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token    \n",
    "    tokens = [word for sent in nltk.sent_tokenize(input_text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z-]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    # Lemma\n",
    "    lemmas = [lemmatizer.lemmatize(t) for t in filtered_tokens]\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Read a txt file and return sentences\n",
    "\"\"\"\n",
    "def read_clean_file(filename):\n",
    "       \n",
    "    with codecs.open(filename, \"r\", encoding='utf-8', errors='ignore') as f:\n",
    "        data = f.read()\n",
    "    if (len(data) < 10):\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    data_cleaned = [] \n",
    "    # remove stop words\n",
    "    data = data.lower()\n",
    "    #data = data.replace('-', ' ')\n",
    "    data = data.replace(',', ' ')\n",
    "    data = data.replace('\\\\', ' ')\n",
    "    data = data.replace('/', ' ')\n",
    "    \n",
    "    for w in data.split():\n",
    "        if (w not in stop_words):\n",
    "            data_cleaned.append(w)\n",
    "    \n",
    "    #print('CLEAN DATA')\n",
    "    #print(' '.join(data_cleaned))\n",
    "    \n",
    "    # lemmatize words in each sentences\n",
    "    data_lemmatized = tokenize_and_lemmatize(' '.join(data_cleaned))\n",
    "    \n",
    "    return data_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-08 09:37:50,092 : INFO : START - Read cleaned wamex reports in /Users/majiga/Documents/wamex/data/wamex_xml/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-834b55da00d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWAMEX_DATA_FOLDER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# do your stuff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mread_clean_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mreports_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_clean_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'END - Read wamex reports'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-69f6cf4facb2>\u001b[0m in \u001b[0;36mread_clean_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0mdata_cleaned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# READ WAMEX REPORTS FROM THE DATA FOLDER\n",
    "logging.info('START - Read cleaned wamex reports in ' + WAMEX_DATA_FOLDER)\n",
    "reports_data = []\n",
    "for filename in glob.glob(os.path.join(WAMEX_DATA_FOLDER, '*.json')):\n",
    "    # do your stuff\n",
    "    if (read_clean_file(filename) is not None):\n",
    "        reports_data.append(read_clean_file(filename).split())\n",
    "logging.info('END - Read wamex reports')\n",
    "\n",
    "# Count the number of tokens\n",
    "count_tokens = 0\n",
    "for data in reports_data:\n",
    "    count_tokens += len(data)\n",
    "print(count_tokens)\n",
    "# 42650553 tokens / 42279106 - without removing hyphens\n",
    "\n",
    "logging.info(\"START - Train the word2vec model\")\n",
    "# The training params are: embedding size: 300, negative samples: 5, window_size: 5,\n",
    "#                       starting learning rate: 0.025 (a dynamic learning rate was used)\n",
    "#                       minimum frequency: 10 (only word occurs more than 10 times were considered).\n",
    "# class gensim.models.word2vec.Word2Vec(sentences=None, size=100, alpha=0.025, window=5, min_count=5,\n",
    "#                           max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001,\n",
    "#                           sg=0, hs=0, negative=5, cbow_mean=1, hashfxn=<built-in function hash>, iter=5,\n",
    "#                           null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=())\n",
    "word2vec_model = gensim.models.Word2Vec(reports_data, window=5, size=100, workers=4, min_count=300, sg=1) \n",
    "\n",
    "print(len(word2vec_model.wv.vocab)) #  8730 tokens\n",
    "print(word2vec_model.wv.vocab)\n",
    "\n",
    "# min_count=100 => training on a 246019540 raw words (182291433 effective words) took 817.9s, 222873 effective words/s if 100+ frequency\n",
    "\n",
    "\n",
    "logging.info('END - word2vec Embeddings for all wamex reports')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.save(MODEL_FILE)\n",
    "\n",
    "# load model\n",
    "new_model = gensim.models.Word2Vec.load(MODEL_FILE)\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.wv.most_similar('gold')\n",
    "\n",
    "#word2vec_model.wv.most_similar('kalgoorlie')\n",
    "\n",
    "word2vec_model.wv.most_similar('iron ore')\n",
    "word2vec_model.wv.most_similar('iron-ore')\n",
    "\n",
    "word2vec_model.wv.most_similar('iron', topn=10)\n",
    "\n",
    "\n",
    "word2vec_model.wv.most_similar(positive=['kalgoorlie','iron-ore'],negative=['gold'])\n",
    "\n",
    "\n",
    "word2vec_model.wv.most_similar(positive=['king','woman'],negative=['man'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
